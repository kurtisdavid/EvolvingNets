{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL REPORT (FR)\n",
    "\n",
    "## Due 29 April 2016 (Friday, 11:59pm)\n",
    "\n",
    "**Important Note:** Before uploading your midterm project on Canvas, please name your file as following:\n",
    "\n",
    "*MT#_FirstLastName.ipynb*\n",
    "\n",
    "where \"#\" denotes the midterm number, \"FirstLastName\" is the name of the student. Students are allowed to work in groups (2 or max. of 3 students). **Each student will hand in their own file**. If you work with another student, please write her/his name on top of the first cell (in a Markdown cell)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question (30 points): CHOOSE YOUR OWN COMPUTATIONAL INTELLIGENCE APPLICATION**\n",
    "\n",
    "In this last exercise, you will choose your own CI application from one of the following main applications (Surrogate-based optimization can be coupled with the other two):\n",
    "\n",
    "* Game AI\n",
    "* 3D Printing\n",
    "* Surrogate-based Optimization\n",
    "\n",
    "You are already familiar with Game AI and 3D printing applications. You can get some ideas about the surrogate-based optimization from the following three papers (you can download them from [UT library](http://www.lib.utexas.edu/) with your EID):\n",
    "\n",
    "* Y. Jin, [A comprehensive survey of fitness approximation in evolutionary computation](http://link.springer.com/article/10.1007%2Fs00500-003-0328-5), Soft Computing, Vol:9, Issue:1, 2005.\n",
    "* A.I.J. Forrester, A.J. Keane, [Recent advances in surrogate-based optimization](http://www.sciencedirect.com/science/article/pii/S0376042108000766), Progress in Aerospace Sciences, Vol:45, 50-79, 2009.\n",
    "* Y. Jin, [Surrogate-assisted evolutionary computation: Recent advances and future challenges](http://www.sciencedirect.com/science/article/pii/S2210650211000198), Swarm and Evolutionary Computation, 61-70, 2011.\n",
    "\n",
    "One of the recent papers that we worked on can be found in this [link](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxjZW1jdHV0dW18Z3g6MmVmY2Q1YjA0ZWVjNzE3MQ).\n",
    "\n",
    "Some other interesting projects could be, but **not limited to**:\n",
    "\n",
    "* Evolutionary multi-objective optimization (EMO) and its applications in games or 3D printing\n",
    "* Evolutionary Many-objective optimization\n",
    "* Use of different evolutionary algorithms: Genetic Programming, Evolution Strategies, Particle Swarm Optimization, Differential Evolution, CMA-ES, Estimation of Distribution Algorithms, etc. (most of these algorithms are avilable in DEAP)\n",
    "* Approximation of 3D objects with cubes, spheres or any base hypercubes using evolutionary algorithms (needs integration of DEAP with OpenSCAD or programming an EA in OpenSCAD)\n",
    "* Designing a 2D car with EAs to finish a given track in shorted amount of time (requires a physics engine)\n",
    "* 3D printable walking or jumping objects (requires a physics engine)\n",
    "* Designing 3D printable accessories using EAs (aesthetic measure is needed for the fitness calculation)\n",
    "* Surrogate-based optimization using a physical simulation or analytical engineering design problem.\n",
    "* Surrogate-based EMO.\n",
    "* Surrogate-based optimization in high-dimensional search space (more than 15 or 20 dimensions).\n",
    "* Robust optimization -- Optimization under uncertainties. For instance, you can investigate the variablity in 3D printing of gears and how to incorporate these variances while designing a reliable gear mechanism\n",
    "* 3D printable lamp design --incorporating variable wall thickness (to control translucency). It may require a physics engine.\n",
    "\n",
    "**IMPORTANT NOTES:** \n",
    "\n",
    "* You can discuss your final project with your friends or mentors, but you have to discuss about it with the instructor before working on it.\n",
    "* Prepare your report in the following format.\n",
    "* Write your report below this cell, not as part of the explanations of format or content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract**:\n",
    "\n",
    "The purpose of this exercise was to write my own implementation of neuroevolution techniques based off of previous projects such as NEAT. To do this, I focused on the concepts of direct encoding, mutations and crossovers of ANN topologies and applications to the common XOR problem. Through working on the project, I was able to learn the mechanics behind NEAT, as well as appreciate its capabilities, as writing speciation is no easy task. The concept of speciation with history markings for ANNs were out of my grasp, but the other techniques such as mutation and crossover could still be applied to solve XOR. I found that certain topologies, even through neuroevolution cannot be further optimized as XOR requires a certain structure to work. The standard no hidden neurons and just input with a bias was not enough, but with the addition of at least one hidden neuron and several connections, XOR was easily solvable.\n",
    "\n",
    "**Introduction**:\n",
    "\n",
    "Standard use of Neural Networks require a fixed topology, where the weights are optimized to obtain desired results. However, much can be gained by optimizing the structure, because certain problems (such as XOR) are only possible with the addition of hidden layers. XOR was used as the test to see how my implementation does, because it is commonly used as a standard basis for neuroevolution (Miikkulainen and Stanley [4], Moore and Spires [2]). By using GA to optimize the topology, rather than having genes that encode weights for each connection, the genes must directly encode for the total structure. Meaning the individuals of the GA will have the information need to create the desired topology (Yao and Lui [6]). Python excels in this, since the GA can easily be transferred using OOP to create these Neural Networks.\n",
    "\n",
    "The main problem at hand was to create an Individual object that would be able to do all of the needed operations for the GA (mutations and crossover) in a uniform manner. In addition, the GA, with the assistance of DEAP, had to be able to implement speciation, as I was basing off my implementation of NEAT. The unique addition they added was that the population during the GA is partitioned into species where only members of the same species could operate on one another [4]. To make this possible, global innovation numbers were developed to have a way to identify certain structures in a topology. Mutation and crossover of individuals will be discussed as well, with a relation to how fitnesses of my implementation were affected by adding certain structures (modifications such as using a sharing function) (Spears [4]). In the end, I will be able to judge how effective my decisions were by implementing the XOR problem to my structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Methodology **\n",
    "\n",
    "To create the individual, I made a Neural Network class that contained several methods and instance variables in order for the GA to work. The instance variables were of course the neurons and connections of the ANN. Each neuron had a separate instance list that contained all of its connections, and all of the connections had an ordered list that stored how the connection is used by two different neurons. Functions such as addNeuron and addConnection were implemented based on the guidelines made in NEAT; neurons could be added between pre-existing connections (since no new input neurons were wanted) and I specifically put extra specifications for the addConnection method. [4] Because I was using recursion to calculate the output (treating the Neural Network as a connected tree), connections could not be added that created infinite loops (where a neuron is connected to a separate neuron that is essential to the input of the original neuron), as well as making sure that multiple direct connections were not possible between neurons (including with itself). I attempted to also implement the innovation number usage, which was successful, to add in speciation to the GA, but I realized that it was much too difficult to finish it in the time allotted, so I focused on these aforementioned methods. Crossover in NEAT actually relied on the global innovation numbers so that individuals with different topologies could still crossover similar structures, but because I was unsuccessful in making speciation work, I had to take a slightly different approach: I similarly used the highest fitness individual as a basis for the new offspring, however, I was only able to crossover the values (by blending the weights together for the given connection) of the initial connections (bias to output or inputs to output). This still provided some way to \"mix\" the two topologies together. Lastly, for all steps of the ANN, the sigmoid function was used as the activation function in this project\n",
    "\n",
    "For the GA, to solve XOR, I actually had two separate GAs: one that optimized the topology, and the other optimizing the weights of the chosen \"best\" topology. I measured fitness over generations by taking the desired outputs given inputs by the XOR problem and finding the total error squared for each trial. Thus, in both GA's I minimized the fitness value for the individuals. One distinct difference between the two is that the former contains specifically individuals of length 1, which contained the instance of an ANN class, while the latter created a static list that contained floating point decimals to represent weights of the desired topology. To run the program, I ran the first GA, optimizing the individual phenotypes. The initial population is in fact made up of only the minimal topologies (bias to output, input to outputs), which corresponds to the argument NEAT made in reducing dimensionality of the search space. [4] This affects the results greatly, because rather than creating random structures, the structures made through the GA will always stem from the same originator. I added adjustments for the fitness (so it wasn't solely the square error), by creating a way to \"penalize\" the addition of structures. Because XOR answers ranged from [-1,1], I had to be very careful in how much fitness the addition of a neuron would augment, but these are what I chose: $fitness = fitness + .000125 * (numHiddenNeurons)^2$. The factor of .000125 was found by going through repeated trials and seeing how it affected the number of hidden neurons being added by the GA. Through experimentation, I noticed that the topologies would exponentially evolve as more and more neurons were added, so I actually had the numHiddenNeurons affect the fitness with order 2, to really limit the possible unhindered growth of topologies. The number of new connections also augmented the fitnesses, by a factor of .0001 linearly, since connections are only possible when new neurons are available, so they have less effect in the evolution. \n",
    "\n",
    "The first GA itself contained 350 generations, in which I recorded the \"evolution\" of topologies every 50 generations to see the changes in the populations' phenotypes. Mutation rate was at .2, in addition to adding a .6 probability to tweak the weights of the ANN or .4 probability to either add a neuron or a connection (which has a .4 and .6 probability, respectively). Crossovers would happen .5 of the time, using the same logic I described previously in this section. And of course, per generation, the fitnesses were calculated and adjusted based on the increased sizes of topologies.\n",
    "\n",
    "The second GA merely takes the resultant topology of the first GA and optimizes the weights to correctly answer XOR. It contained 500 generations (which would often be more than enough for the correct topology) keeping mutation and crossover rates constant from before. It utilized different methods for the GA, but they are trivial in that they are just standard functions used for real valued individuals (including tournamentSelect). All the specifications are located in the code attached with the report. \n",
    "\n",
    "The results in the next section were found by running 3 trials of this process, and plotting the corresponding values. In addition, I took excerpts of evolving structures of a certain topology to show how it changed from generation to generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Results and Discussions **\n",
    "\n",
    "The results will start with an example progression of an actual population using my 2 GA's! The generations of each are 50, 150, 350 and 350 (after GA2) respectively downwards.\n",
    "\n",
    "<img src=\"Topology 1.png\">\n",
    "\n",
    "<img src=\"Topology 2.png\">\n",
    "\n",
    "<img src=\"Topology 3.png\">\n",
    "\n",
    "<img src=\"Topology 4.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "As you can see, it started from the stated minimal topology, but as the generation number increased, it began to converge closer and closer to the optimal solution for the XOR problem (Tuhársky, Smolár, Reiff, Kuzma, Eperješi, Sinčák 2010 [5]): \n",
    "\n",
    "<img src=\"Solution.png\"> \n",
    "\n",
    "In fact, the final topology given had a fitness of .01, where the connections resulted in the following statements:\n",
    "\n",
    "0 XOR 0 = 0.0800\n",
    "\n",
    "0 XOR 1 = 0.9408\n",
    "\n",
    "1 XOR 0 = 0.9863\n",
    "\n",
    "1 XOR 1 = 0.0800\n",
    "\n",
    "So the end result was very close to what was desired out of XOR.\n",
    "\n",
    "Following are graphs that depict the relationship between the topologies in GA 1 and the rate at which the fitnesses were minimized by GA2:\n",
    "\n",
    "<img src=\"Topological Data.png\">\n",
    "\n",
    "<img src=\"Weight Optimization.png\">\n",
    "\n",
    "\n",
    "I first want to look at Trial 3, which had the most unique results. From the first figure, we see that Trial 3 only had a maximum of 1 hidden neuron over 350 generations, in addition to having only one more addition connection from the initial topology. We see the drastic effect this takes in the second figure, where unlike the other 2 graphs, the fitnesses were unable to be minimized even after 500 generations, most likely being that the overall structure for that trial was not sufficient for the XOR problem. Looking at Trials 1 and 2, we see that they are more \"successful\" since they eventually reach a minimum value (where Trial 1 actually did the best). At first, one may think that the Trial that resulted in the most number of neurons/connections would most quickly be optimized, but in fact, Trial 1 had JUST as much hidden neurons as the 3rd trial (1) but was able to create the connections needed to increase the effectiveness of the topology for XOR. Meanwhile, Trial 2 resulted in an addition of 4 new hidden neurons, and OVER 9 additional connections which would result in an overly complicated topology. This simplicity of Trial 1 made it much easier to optimize with GA, having a very nice curvature of optimization, whereas Trial 2 had more abrupt changes as the generations increased. Overall, even though not all the trials were successful in solving the XOR problem, this shows that evolving topologies can in fact reach the optimal solution and thus reduce the amount of prior experimentation to obtain the required topology for a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Conclusion: ** \n",
    "\n",
    "Given the time for this final project, I find that the work and effort I put in to understand the basics of NEAT and other neuroevolutionary techniques will serve me a great deal in the long run, because I understand the basis of these ideas much more. My implementation was not a complete failure, because with the help of using 2 GAs, I was not only able to optimize a structure specifically for XOR, but obtain accurate results as well. One thing I want to work on in the future, however, is to find a way to implement the speciation with innovation numbers, because I feel that this method would more clearly converge to a certain topology against most trials. In addition, for my project next semester, I am still interested in finding a way to connect NEAT with the game interface of League of Legends in order to optimize the movements of a character. I have found a paper done by other students that explain the mathematics behind certain movements made based on the role of a certain character, but I want to work more on the AI side of finding the best possible route in every situation. (Caligaris, Isa, Daher, Veer, and Kharma 2013 [1]) This idea is stemmed from the application of MarI/O with NEAT, with some application to surrogate based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** References **\n",
    "\n",
    "[1] Caligaris, Daher, Kharma, & Veer. (2013). Optimizing Jungle Paths in League of Legends. CMU Math Department.\n",
    "\n",
    "\n",
    "[2] Moore, B., & Spires, W. (2006). Signal Complexification using Frequency Modulation and Neuroevolution. doi:10.1.1.134.3093\n",
    "\n",
    "\n",
    "[3] Spears, W. (1995). Speciation using tag bits. In Handbook of Evolutionary Computation. IOP Publishing Ltd. and Oxford University Press, Oxford, UK.\n",
    "\n",
    "[4] Stanley, K. O., & Miikkulainen, R. (2002). Evolving Neural Networks through Augmenting Topologies. Evolutionary Computation, 10(2), 99-127. \n",
    "doi:10.1162/106365602320169811\n",
    "\n",
    "[5] Tuhársky, Smolár, Reiff, Kuzma, Eperješi, & Sinčák. (2010). Evolutionary approach for structural and parametric adaptation of NN for XOR problem. Dept. of Cybernetics and Artificial Intelligence, FEI TU of Košice, Slovak Republic.\n",
    "\n",
    "[6] Yao, X. and Shi, Y. (1995). A preliminary study on designing artificial neural networks using coevolution.\n",
    "In Proceedings of the IEEE Singapore International Conference on Intelligent Control\n",
    "and Instrumentation, pages 149–154, IEEE Singapore Section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
